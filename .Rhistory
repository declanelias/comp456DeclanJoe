data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot(model=TRUE)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot(rountint=FALSE)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
View(fit_mod)
knitr::opts_chunk$set(echo = TRUE)
install.packages("shiny")
library(shiny)
runExample("01_hello")
library(shiny)
ui <- fluidPage("")
server <- function(input, output) {}
shinyApp(ui = ui, server = server)
ui <- fluidPage("KOBE")
ui <- fluidPage("KOBE")
library(shiny)
ui <- fluidPage("")
server <- function(input, output) {}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage("KOBE")
server <- function(input, output) {}
shinyApp(ui = ui, server = server)
library(babynames)
babynames
View(fit_mod)
ct_spec <- decision_tree(complexity = 0.5) %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
ct_spec <- decision_tree(cost_complexity = 0.5) %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree(cost_complexity = 0.75) %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree(cost_complexity = 0.9) %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree(cost_complexity = 0.1) %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.01,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.01,  #default is 0.01 (used for pruning a tree)
min_n = 6, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = 6, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
#min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
set.seed(121)
load("NFL_movement_split_df.rdata")
NFL_Full <- NFL_lines_split %>%
select(-Index, -inserted, -league, -id, -Final_line, -public_team_under) %>%
mutate(book_id = as.factor(book_id)) %>%
filter(book_id != "69")%>%
na.omit()
NFL_Full %>%
group_by(factor(make_bet)) %>%
summarise(t=n())
NFL_lines_split %>%
group_by(book_id) %>%
summarise(t = n())
# Make sure you set reference level (the outcome you are NOT interested in)
NFL_Full <- NFL_Full %>%
mutate(outcome = relevel(factor(make_bet), ref='No')) #set reference level
NFL_cv10 <- vfold_cv(NFL_Full, v = 10)
# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# Recipe
logistic_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full, family = binomial('logit'), maxit = 100) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
# Workflow (Recipe + Model)
log_wf <- workflow() %>%
add_recipe(logistic_rec) %>%
add_model(logistic_spec)
# Fit Model to Training Data
log_fit <- fit(log_wf, data = NFL_Full)
# Print out Coefficients
log_fit %>% tidy()
# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
mutate(OR = exp(estimate))
# Make soft (probability) predictions
predict(log_fit, new_data = NFL_Full, type = "prob")
# Make hard (class) predictions (using a default 0.5 probability threshold)
predict(log_fit, new_data = NFL_Full, type = "class")
# Soft predictions
logistic_output <-  NFL_Full %>%
bind_cols(predict(log_fit, new_data = NFL_Full, type = 'prob'))
# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
mutate(.pred_class = make_two_class_pred(.pred_No, levels(outcome), threshold = .60)) #Try changing threshold (.5, 0, 1, .2, .8)
# Visualize Soft Predictions
logistic_output %>%
ggplot(aes(x = outcome, y = .pred_Yes)) +
geom_boxplot() +
geom_hline(yintercept = 0.60, color='red') +  # try changing threshold
labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
theme_classic()
# Confusion Matrix
logistic_output %>%
conf_mat(truth = outcome, estimate = .pred_class)
log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions
#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome
logistic_output %>%
log_metrics(estimate = .pred_class, truth = outcome, event_level = "second") # set second level of outcome as "success"
logistic_roc <- logistic_output %>%
roc_curve(outcome, .pred_Yes, event_level = "second") # set second level of outcome as "success"
autoplot(logistic_roc) + theme_classic()
# CV Fit Model
log_cv_fit <- fit_resamples(
log_wf,
resamples = NFL_cv10,
metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations
collect_metrics(log_cv_fit) #default threshold is 0.5
rf_OOB_output(data_fit_mtry4,4, NFL_Full %>% pull(make_bet)) %>%
conf_mat(truth = make_bet, estimate= .pred_class)
model_output <-data_fit_mtry4 %>%
extract_fit_engine()
model_output %>%
vip(num_features = 30) + theme_classic() #based on impurity
model_output %>% vip::vi() %>% head()
model_output2 <- data_wf_mtry4 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = NFL_Full) %>%
extract_fit_engine()
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.00000001,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot(model = TRUE)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.001,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.01,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.015,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.0075,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.005,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
model_output2 <- data_wf_mtry4 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = NFL_Full) %>%
extract_fit_engine()
model_output2 %>%
vip(num_features = 30) + theme_classic()
model_output2 %>% vip::vi() %>% head()
install.packages("rfviz")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
library(rfviz)
