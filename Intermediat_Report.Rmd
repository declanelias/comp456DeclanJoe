---
title: 'Taking the Sports out of Sports Gambling: Arbitrage and Expected Value Betting'
author: "Joe Margolis and Declan Elias"
date: '2022-11-14'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
library(rfviz)
library(tree.interpreter)
library(kableExtra)
```

## Introduction

### Topic

A rapid occurrence across the United States that is currently taking place is the continuous legalization of sports gambling passing through state governments. With 30 states already passing laws allowing sports betting, the business itself has exploded as it has even become a major area of investment for the sports franchises themselves, as they open up books in stadiums and partnerships with books are being put in place. This rapid growth in influence of sports betting, along with Declan Elias and my interests in statistical analysis and sports in general is the reason why we have decided to explore this topic in this report.

### Research Questions

Our research questions revolve around finding two ways to maximize expected value of betting, and doing so in a way to where over time a gambler can actually expect to slowly earn a profit. The idea is treating sports gambling more like stock trade as opposed to game, finding comparisons within games and books in order to place bets with opportunity for slow success.

#### Expected Value Across Books

The first way we attempt to profit off of expected value is by comparing a certain game across books. There are hundreds of books out there, and while there is plenty of model sharing and line sharing amongst them, there are still many instances where books produce different lines for the same game. With this in mind we are attempting to put together a model that weighs the success of each book differently based on their history, and use that to set a "gold standard" line for any particular game. This would hopefully allow us to have a consistent baseline model that we can count as an even expected value, and then compare book lines to that model to see which books can give us a positive expected value.

#### Expected Value As Lines Move

The other way we are investigating in order to leverage expected value betting is through the way that lines can change within a given book. In many sports, particularly sports like football which only play once a week, we can notice an abundance of changes in the lines a book offers from the first line they set up until game time, when the book has the most knowledge. Books will try to adjust lines in order to end up with as close to a 50/50 split on all bets from the public. What we are attempting with this is to model how these lines tend to move, leading to a classification output where we decide if a given line is going to be better or worse than its closing value, allowing a gambler to decide if a current bet is a good idea.

## Data 

## Predicting For Closing Line Value

### Major League Baseball (MLB)

### National Football League (NFL)

```{r}
set.seed(121)
load("NFL_movement_split_df.rdata")
NFL_Full <- NFL_lines_split %>%
  select(-Index, -inserted, -league, -id, -Final_line, -public_team_under, -book_id) 
```

#### Logistic Analysis

##### Model Setting

```{r}
# Make sure you set reference level (the outcome you are NOT interested in)
NFL_Full <- NFL_Full %>%
  mutate(outcome = relevel(factor(make_bet), ref='No')) #set reference level

NFL_cv10 <- vfold_cv(NFL_Full, v = 10)

# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location, data = NFL_Full, family = binomial('logit'), maxit = 100) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# Fit Model to Training Data
log_fit <- fit(log_wf, data = NFL_Full)
```

##### Checking Model

```{r}
# Print out Coefficients
log_fit %>% tidy() %>%
  kbl(caption = "Logistic Model Coefficients") %>%
  kable_classic(full_width = F, html_font = "Cambria")

# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate)) %>%
  kbl(caption = "Logistic Model Exponentiated Coefficients and Confidence Interval") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

##### Making Predictions 

```{r, results='hide'}
# Make soft (probability) predictions
predict(log_fit, new_data = NFL_Full, type = "prob")

# Make hard (class) predictions (using a default 0.5 probability threshold)
predict(log_fit, new_data = NFL_Full, type = "class")
```

##### Training Data Predictions

```{r, warning=FALSE}
# Soft predictions
logistic_output <-  NFL_Full %>%
  bind_cols(predict(log_fit, new_data = NFL_Full, type = 'prob')) 

# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_No, levels(outcome), threshold = .60)) #Try changing threshold (.5, 0, 1, .2, .8)

# Visualize Soft Predictions
logistic_output %>%
  ggplot(aes(x = outcome, y = .pred_Yes)) +
  geom_boxplot() + 
  geom_hline(yintercept = 0.60, color='red') +  # try changing threshold
  labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
  theme_classic()
```

##### Evaluating Model

```{r}
# Confusion Matrix
conf <- logistic_output %>%
  conf_mat(truth = outcome, estimate = .pred_class)

log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions

#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome

logistic_output %>% 
  log_metrics(estimate = .pred_class, truth = outcome, event_level = "second")  %>%
  kbl(caption = "Logistic Model Metric Results") %>%
  kable_classic(full_width = F, html_font = "Cambria") # set second level of outcome as "success"
```

##### ROC Curve

```{r}
logistic_roc <- logistic_output %>% 
    roc_curve(outcome, .pred_Yes, event_level = "second") # set second level of outcome as "success"

autoplot(logistic_roc) + theme_classic()
```
##### CV Evaluation of Model

```{r}
# CV Fit Model
log_cv_fit <- fit_resamples(
    log_wf, 
    resamples = NFL_cv10,
    metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations

collect_metrics(log_cv_fit) %>% #default threshold is 0.5
  kbl(caption = "Logistic Model Metric CV Fold Test Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```